{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02fd3ec4",
   "metadata": {},
   "source": [
    "## 3.2. Evaluación de la eficacia del modelo\n",
    "En este libro practicaremos con los principales métodos para medir la eficacia o el rendimiento de nuestros modelos de aprendizaje. \n",
    "\n",
    "Realizaremos las distintas pruebas sobre la base de datos IRIS: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "fbca3d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "\n",
    "# Cargamos el conjunto de datos de IRIS\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fd339a",
   "metadata": {},
   "source": [
    "### 3.2.1. Hold-out\n",
    "La forma más sencilla de particionar nuestros datos de entrenamiento y prueba es a través de un hold-out. El método de sklearn llamado train_test_split nos proporciona esta funcionalidad. Este método recibe como parámetros los conjuntos a particionar, además del parámetro test_size con el que establece el tanto por uno de datos que formarán parte del conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "01d5a604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 4) (90,)\n",
      "(60, 4) (60,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffdced7",
   "metadata": {},
   "source": [
    "A continuación, entrenaremos una máquina de soporte vectorial (SVM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8a23d501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)   # Nota: fijamos el valor de C=1 para este ejemplo\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a6fcb5",
   "metadata": {},
   "source": [
    "Cuando se evalúan diferentes hiperparámetros para los predictores, como el hiperparámetro C de la SVM, sigue existiendo  riesgo de sobreajuste en el conjunto de prueba porque los parámetros pueden ajustarse hasta que el estimador tenga un rendimiento óptimo. De este modo, el conocimiento sobre el conjunto de pruebas puede \"filtrarse\" en el modelo y las métricas de evaluación ya no informan sobre el rendimiento de la generalización. Para resolver este problema, se puede reservar otra parte del conjunto de datos, a la cual se llama \"conjunto de validación\": el entrenamiento se lleva a cabo en el conjunto de entrenamiento, después se realiza la evaluación en el conjunto de validación y, cuando el experimento parece tener éxito, se puede realizar la evaluación final en el conjunto de prueba.\n",
    "\n",
    "Sin embargo, al dividir los datos disponibles en tres conjuntos, reducimos drásticamente el número de muestras que pueden utilizarse para el aprendizaje del modelo, y los resultados pueden depender de una elección aleatoria concreta para el par de conjuntos (entrenamiento, validación).\n",
    "\n",
    "### 3.2.2. Cross-validation\n",
    "\n",
    "Una solución a este problema es un procedimiento denominado validación cruzada (CV, por sus siglas en inglés). El conjunto de prueba debe seguir utilizándose para la evaluación final, pero el conjunto de validación ya no es necesario cuando se realiza la CV. En el enfoque básico, denominado validación cruzada en $k$-pliegues ($k$-fold cross-validation), el conjunto de entrenamiento se divide en $k$ conjuntos más pequeños (más adelante se describen otros enfoques, pero en general siguen los mismos principios). Se sigue el siguiente procedimiento para cada uno de los $k$ \"pliegues\":\n",
    "\n",
    "- Se entrena un modelo utilizando $k-1$ pliegues como datos de entrenamiento;\n",
    "\n",
    "- el modelo resultante se valida con la parte restante de los datos (es decir, se utiliza como conjunto de prueba para calcular una medida de rendimiento, como la precisión).\n",
    "\n",
    "La medida de rendimiento obtenida mediante la validación cruzada en $k$-pliegues es la media de los valores calculados en el bucle. Este método puede ser costoso desde el punto de vista computacional, pero no desperdicia demasiados datos (como ocurre cuando se fija un conjunto de validación arbitrario), lo que supone una gran ventaja en problemas en los que el número de muestras es pequeño.\n",
    "\n",
    "#### 3.2.2.1. Función cross_val_score\n",
    "\n",
    "La forma más sencilla de utilizar la validación cruzada es llamar a la función de ayuda cross_val_score sobre el estimador y el conjunto de datos.\n",
    "\n",
    "El siguiente ejemplo muestra cómo estimar la precisión de una SVM con kernel lineal en el conjunto de datos iris,dividiendo los datos, ajustando un modelo y calculando la puntuación 5 veces consecutivas (con diferentes divisiones (splits) cada vez):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "84632a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96666667 1.         0.96666667 0.96666667 1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "clf = svm.SVC(kernel='linear', C=1, random_state=42)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c69a8b",
   "metadata": {},
   "source": [
    "La puntuación media y la desviación típica vienen dadas por:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "95b8984b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La CV obtuvo una accuracy media de 0.980 con desviación estándar igual a 0.016\n"
     ]
    }
   ],
   "source": [
    "print(\"La CV obtuvo una accuracy media de {:.3f} con desviación estándar igual a {:.3f}\".format(scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c55eec",
   "metadata": {},
   "source": [
    "Por defecto, la puntuación calculada en cada iteración de la CV es el método de puntuación del predictor. Es posible cambiarlo utilizando el parámetro scoring y, por ejemplo, utilizar la macro-F1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "4200103b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96658312 1.         0.96658312 0.96658312 1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "scores = cross_val_score(clf, X, y, cv=5, scoring='f1_macro')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740acc89",
   "metadata": {},
   "source": [
    "Consultar el siguiente <a href='https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter'>enlace</a> para ver el listado detallado de las métricas de evaluación que se pueden utilizar.\n",
    "\n",
    "Se puede apreciar que los resultados utilizando la accuracy y la macro-F1 son iguales. Ello se debe a que los ejemplos están balanceados en base a su clase. Cuando el parámetro cv de la función cross_val_score es un entero, ésta utiliza la estrategia KFold o StratifiedKFold para particionar los datos. La elección depende de si el predictor deriva de la clase ClassifierMixin. Sklearn permite usar más estrategias. Para ello, basta con indicarla en el parámetro cv:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "8ded3cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97777778 0.97777778 1.         0.95555556 1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=cv)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91399293",
   "metadata": {},
   "source": [
    "Del mismo modo que es importante probar un predictor con datos que no se han tenido en cuenta en el entrenamiento, el preprocesamiento (como la estandarización, la selección de características, etc.) y otras transformaciones de datos similares deben aprenderse de un conjunto de entrenamiento y aplicarse a los datos que no se han tenido en cuenta para la predicción. Por ejemplo, el siguiente código nos muestra cómo se podría hacer este procedimiento cuando se aplica a un conjunto de entrenamiento y de prueba (en CV habría que hacer esto mismo en cada uno de los pliegues):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8f98b8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "clf = svm.SVC(C=1).fit(X_train_transformed, y_train)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "score = clf.score(X_test_transformed, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4b8a43",
   "metadata": {},
   "source": [
    "Un Pipeline facilita la composición de estimadores, proporcionando este comportamiento bajo validación cruzada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "3302f91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97777778 0.93333333 0.95555556 0.93333333 0.97777778]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "clf = make_pipeline(preprocessing.StandardScaler(), svm.SVC(C=1))\n",
    "scores = cross_val_score(clf, X, y, cv=cv)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeb5197",
   "metadata": {},
   "source": [
    "#### 3.2.2.2. Función cross_validate\n",
    "\n",
    "Una alternativa a cross_val_score es la función cross_validate, que difiere de en dos aspectos:\n",
    "- Permite especificar múltiples métricas para la evaluación.\n",
    "- Devuelve un dict que contiene tiempos de ajuste, tiempos de puntuación (y opcionalmente puntuaciones de entrenamiento así como estimadores ajustados) además de la puntuación de la prueba.\n",
    "\n",
    "Se puede establecer el parámetro return_train_score a verdadero para evaluar las puntuaciones en el conjunto de entrenamiento.\n",
    "También puede conservar el estimador ajustado en cada conjunto de entrenamiento estableciendo return_estimator=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f15b5e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro']\n",
      "[0.96666667 1.         0.96666667 0.96666667 1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score\n",
    "scoring = ['precision_macro', 'recall_macro']\n",
    "clf = svm.SVC(kernel='linear', C=1, random_state=0)\n",
    "scores = cross_validate(clf, X, y, scoring=scoring)\n",
    "print(sorted(scores.keys()))\n",
    "print(scores['test_recall_macro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d37b86",
   "metadata": {},
   "source": [
    "#### 3.2.2.3. Iteradores\n",
    "\n",
    "En ocasiones trabajaremos con predictores propios o de una librería diferente a sklearn. Para estos casos, el uso de las funciones anteriores no es, en principio, aplicable. Sin embargo, sí podemos hacer uso de los mecanismos que nos proporcionan los iteradores para definir los pliegues. \n",
    "\n",
    "##### 3.2.2.3.1. KFold\n",
    "\n",
    "KFold divide el conjunto de muestras en grupos, llamados pliegues, de igual tamaño (si es posible). La función de predicción se aprende utilizando $k-1$ pliegues, y el pliegue que queda fuera se utiliza para la prueba.\n",
    "\n",
    "Ejemplo de validación cruzada de 2 pliegues en un conjunto de datos con 4 muestras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "6264446f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3] [0 1]\n",
      "[0 1] [2 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = [\"a\", \"b\", \"c\", \"d\"]\n",
    "kf = KFold(n_splits=2)\n",
    "splits = kf.split(X)\n",
    "for train, test in splits:\n",
    "    print('{} {}'.format(train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb0b391",
   "metadata": {},
   "source": [
    "Cada pliegue está constituido por dos matrices de índices: la primera con los elementos del conjunto de entrenamiento y la segunda con los del conjunto de prueba. Así, se pueden crear los conjuntos de entrenamiento/prueba utilizando la indexación numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "5edaf443",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train, test in splits:\n",
    "    X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5894f2b5",
   "metadata": {},
   "source": [
    "##### 3.2.2.3.2. Stratified KFold\n",
    "\n",
    "StratifiedKFold es una variación de k-fold que devuelve pliegues estratificados: cada conjunto contiene aproximadamente el mismo porcentaje de muestras de cada clase objetivo que el conjunto completo.\n",
    "\n",
    "A continuación se muestra un ejemplo de validación cruzada estratificada triple en un conjunto de datos con 50 muestras de dos clases desequilibradas. Se muestra el número de muestras de cada clase y se compara con KFold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c011b143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified KFold:\n",
      "train -  [30  3]   |   test -  [15  2]\n",
      "train -  [30  3]   |   test -  [15  2]\n",
      "train -  [30  4]   |   test -  [15  1]\n",
      "KFold:\n",
      "train -  [28  5]   |   test -  [17]\n",
      "train -  [28  5]   |   test -  [17]\n",
      "train -  [34]   |   test -  [11  5]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "X, y = np.ones((50, 1)), np.hstack(([0] * 45, [1] * 5))\n",
    "\n",
    "print('Stratified KFold:')\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "for train, test in skf.split(X, y):\n",
    "    print('train -  {}   |   test -  {}'.format(np.bincount(y[train]), np.bincount(y[test])))\n",
    "\n",
    "print('KFold:')\n",
    "kf = KFold(n_splits=3)\n",
    "for train, test in kf.split(X, y):\n",
    "    print('train -  {}   |   test -  {}'.format(np.bincount(y[train]), np.bincount(y[test])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044e69f6",
   "metadata": {},
   "source": [
    "##### 3.2.2.3.3. Repeated KFold\n",
    "\n",
    "RepeatedKFold repite el K-Fold $n$ veces (idemo para StratifiedRepeatedKFold). Se puede utilizar cuando se requiere ejecutar KFold $n$ veces, produciendo diferentes divisiones en cada repetición.\n",
    "\n",
    "Ejemplo de K-Fold repetido 2 veces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "97a98617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3] [0 1]\n",
      "[0 1] [2 3]\n",
      "[0 2] [1 3]\n",
      "[1 3] [0 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "random_state = 12883823\n",
    "rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=random_state)\n",
    "for train, test in rkf.split(X):\n",
    "    print('{} {}'.format(train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650ae9e5",
   "metadata": {},
   "source": [
    "De forma similar, el RepeatedStratifiedKFold repite el Stratified K-Fold $n$ veces con diferente aleatorización en cada repetición."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ba23ca",
   "metadata": {},
   "source": [
    "##### 3.2.2.3.4. Leave One Out \n",
    "\n",
    "LeaveOneOut es una validación cruzada simple. Cada conjunto de aprendizaje se crea tomando todas las muestras excepto una, siendo el conjunto de prueba la muestra dejada fuera. Así, para n muestras, tenemos n conjuntos de entrenamiento diferentes y n conjuntos de prueba diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ae42b960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3] [0]\n",
      "[0 2 3] [1]\n",
      "[0 1 3] [2]\n",
      "[0 1 2] [3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "X = [1, 2, 3, 4]\n",
    "loo = LeaveOneOut()\n",
    "for train, test in loo.split(X):\n",
    "    print('{} {}'.format(train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c310af5d",
   "metadata": {},
   "source": [
    "##### 3.2.2.3.5. Group k-fold\n",
    "\n",
    "GroupKFold es una variación de $k$-fold cross-validation que garantiza que el mismo grupo no esté representado tanto en los conjuntos de prueba como en los de entrenamiento. Por ejemplo, si los datos se obtienen de diferentes sujetos con varias muestras por sujeto y si el modelo es lo suficientemente flexible como para aprender de características muy específicas de la persona, podría fallar a la hora de generalizar a nuevos sujetos. GroupKFold permite detectar este tipo de situaciones de sobreajuste.\n",
    "\n",
    "Imagine que tiene tres sujetos, cada uno con un número asociado del 1 al 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "31c43659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5] [6 7 8 9]\n",
      "[0 1 2 6 7 8 9] [3 4 5]\n",
      "[3 4 5 6 7 8 9] [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9, 10]\n",
    "y = [\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"d\", \"d\", \"d\"]\n",
    "groups = [1, 1, 1, 2, 2, 2, 3, 3, 3, 3]\n",
    "\n",
    "gkf = GroupKFold(n_splits=3)\n",
    "for train, test in gkf.split(X, y, groups=groups):\n",
    "    print('{} {}'.format(train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593c606f",
   "metadata": {},
   "source": [
    "Cada sujeto está en un pliegue de prueba diferente, y el mismo sujeto nunca está tanto en el de prueba como en el de entrenamiento. Observe que los pliegues no tienen exactamente el mismo tamaño debido al desequilibrio de los datos. La misma estrategia es aplicable al leave-one-out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "444c4a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 5 6 7 8 9] [0 1 2]\n",
      "[0 1 2 6 7 8 9] [3 4 5]\n",
      "[0 1 2 3 4 5] [6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "logo = LeaveOneGroupOut()\n",
    "for train, test in logo.split(X, y, groups=groups):\n",
    "    print('{} {}'.format(train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1790a68",
   "metadata": {},
   "source": [
    "Si las proporciones de clase deben estar equilibradas en todos los pliegues, StratifiedGroupKFold es una mejor opción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "c42bd7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  3  4  5  6  7 10 11 15 16 17] [ 1  8  9 12 13 14]\n",
      "[ 0  1  4  5  6  7  8  9 11 12 13 14] [ 2  3 10 15 16 17]\n",
      "[ 1  2  3  8  9 10 12 13 14 15 16 17] [ 0  4  5  6  7 11]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "X = list(range(18))\n",
    "y = [1] * 6 + [0] * 12\n",
    "groups = [1, 2, 3, 3, 4, 4, 1, 1, 2, 2, 3, 4, 5, 5, 5, 6, 6, 6]\n",
    "sgkf = StratifiedGroupKFold(n_splits=3)\n",
    "for train, test in sgkf.split(X, y, groups=groups):\n",
    "    print('{} {}'.format(train, test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6cadc2",
   "metadata": {},
   "source": [
    "#### 3.2.2.4. Validación cruzada para series temporales\n",
    "\n",
    "Las series temporales se caracterizan por la correlación entre observaciones cercanas en el tiempo (autocorrelación). Sin embargo, las técnicas clásicas de validación cruzada, como KFold y ShuffleSplit, suponen que las muestras son independientes y se distribuyen de forma idéntica, y darían lugar a una correlación poco razonable entre las instancias de entrenamiento y de prueba (lo que produciría estimaciones deficientes del error de generalización) en los datos de series temporales. Por lo tanto, es muy importante evaluar nuestro modelo para datos de series temporales en las observaciones \"futuras\" menos parecidas a las que se utilizan para entrenar el modelo. \n",
    "\n",
    "TimeSeriesSplit es una variación de k-fold que devuelve los primeros pliegues como conjunto de entrenamiento y el tercer pliegue como conjunto de prueba. A diferencia de los métodos estándar de validación cruzada, los conjuntos de entrenamiento sucesivos son superconjuntos de los anteriores. Además, añade todos los datos sobrantes a la primera partición de entrenamiento, que siempre se utiliza para entrenar el modelo.\n",
    "\n",
    "Ejemplo de validación cruzada de 3 particiones de series temporales en un conjunto de datos con 6 muestras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "037ac21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None)\n",
      "[0 1 2] [3]\n",
      "[0 1 2 3] [4]\n",
      "[0 1 2 3 4] [5]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([1, 2, 3, 4, 5, 6])\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "print(tscv)\n",
    "TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None)\n",
    "for train, test in tscv.split(X):\n",
    "    print('{} {}'.format(train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2ccf91",
   "metadata": {},
   "source": [
    "### 3.2.3. Bootstrap\n",
    "\n",
    "Este método consiste en crear, en cada una de las iteraciones del método, un conjunto de entrenamiento y otro de prueba. El conjunto de entrenamiento, del mismo tamaño que el conjunto original, se obtiene a partir de un muestreo con reemplazamiento, de forma que podrá contener elementos duplicados. El conjunto de prueba estará formado por los elementos no seleccionados del conjunto original. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "09e6e076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "X = np.array([[0., 0.], [1., 1.], [2., 2.], [3., 3.], [4., 4.], [5., 5.], [6., 6.], [7., 7.]])\n",
    "y = np.array([0, 1, 2, 3, 4, 5, 6, 7])\n",
    "\n",
    "n_iterations = 10\n",
    "for i in range(n_iterations):\n",
    "    X_train, y_train = resample(X, y)\n",
    "    # Seleccionamos para el conjunto de prueba los elementos no seleccionados en X_train\n",
    "    X_test, y_test = [], []\n",
    "    for i in range(len(X)): \n",
    "        if X[i] not in X_train:\n",
    "            X_test.append(X[i].tolist())\n",
    "            y_test.append(y[i].tolist())\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "    # A partir de aquí ajustaríamos el predictor/clasificador\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
